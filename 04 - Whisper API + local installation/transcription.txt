Good evening, guys. Evening. Hello. Do you choose Mr. Sturgis all or not yet? Is there anyone who hasn't sent out? Maybe two, three minutes ago. Today's lecture. Today's the deadline, yeah? By the way, do we have strict deadlines for homework, guys? No. Isn't it that strict? No, no, no, it's not strict. But I would suggest to find at least like 10 to 15 minutes every day how I am doing. Because when it will come at the end, it will be like everything. So at least I'm just trying to find 10, 15, 20 minutes doing like by small, small, small parts. Then every day. Homework from this course or from others? Any, I think. Other courses, I think, didn't give you homework yet, yeah? They give it, they give it. Data, data processing, they gave, it gave, he gave yesterday, I think, this file format. I, yesterday, I missed the class yesterday. I will watch it today. Hey, guys. Hello, Italy. Hello, Italy. How are you doing? I have a question about the future. If you see, two days ago, Google introduced quantum computers. Yeah. In our second lesson, you said that there is a problem with the model because they have a resource problem. And what will be in the future about it? Because if Google, if we have, they have quantum computers, then they have a huge resource. And how they will change. Yeah. So that's, that's really good. That's an interesting question. So I didn't know how they work, like on the very general concept of these qubits. I still, you know, I've been studying quantum physics in university, like, at least one year, probably two years. I can't remember. You know, a lot of electronics works on quantum effects. So, for example, some, some stuff allows the electrons to jump the barrier. But they, like, actually cannot jump. So they, like, tunneling through that. And no one knows, like, how it works. But it works. And we use it in TVs and smartphones. And that's it. Like, some kind of, some kind of magic. So it's still, I think there should be some kind of scientific explanation. But I didn't know, like, how these quantum computers work. I don't understand the concept of, like, something having the value of zero and one at the same time. Because, like, okay, you can imagine that. But how about the storage, right? So if you talk about, like, HDD or SSD, any storage, so you need to write it. And you need to, like, you should be able to read it in the future. And the most fear, like, two fears I remember, like, from the community regarding quantum computers is, okay, if they are so, like, powerful. And can make, like, much more, like, I don't know, millions times computations per second. So what about the encryption, right? So RSA or any other encryption algorithm. So basically encryption works, like, modern encryption works on the very basic example. Like, there are, how it's called, like, one-way function in math. I don't know how it's in English. So the way, like, it's very easy to calculate this. For example, it's very easy to multiply two prime integers and get the result. But it's super hard to understand, like, what are the integers you multiplied if you received, like, this result. And a lot of encryption is based on that. And actually Bitcoin is based on that. So Bitcoin has several, like, Bitcoin has a lot of protocols inside. Like, technologies, and most of them are, like, top-tier encryption we have right now. So if Google can beat that, so they can possibly, like, hijack all the new Bitcoins. Maybe they can rewrite the whole Bitcoin, like, the whole blockchain, like, in a day. And, I don't know, like, steal all the Bitcoins from us. So a lot of, like, military applications, like, all this encryption and networking. So, I don't know. It's technically possible that, yeah, such technology can appear. But taking into account how much time and money and resources and smart people it takes to build such thing. So maybe the first country which builds it, I don't know, like, will create some kind of, like, snow crash. Like, from cyberpunk books. I don't know. So, we'll see. At least, like, I know, like, from what I understand, like, IT history and technology history. Like, all the new, like, top-tier technologies, they have, like, two applications. Like, the two industries basically driving them fast and, like, making them broadly available is porn industry and military. So, this is, like, two things usually picking up the latest tech and trying to get, like, money of that. And maybe one of them will pick it up. I don't know. Maybe military first. So, we're going to see the new applications of quantum stuff. Yeah, but from personal perspective, like, being a human, like, I don't know, like, IT guy. So, yeah, that's interesting. But unless I cannot benefit from that, unless I cannot, like, earn money doing that. So, it's inevitable for me. Like, the sun. The sun just can stop. I don't know. Like, okay, bad example because the earth rotates across the sun. So, the earth can just stop rotating and it will be an infinite night for me. So, I can't do anything about that. I can't do anything about rain. So, what I can do is just to buy the umbrella or, I don't know, pray the rain stops sometime. And in case I have the problem or something I cannot influence, I think it's good not to worry about it. Because if you worry about something you cannot change, that's bad. So, you spent a lot of resources and the result will be nothing, like, for sure. So, my plan is just read the news, I don't know, follow this agenda, but don't worry about it. Okay. Yeah, let's get down to business. So, let me try to open my schedule. I had to reboot my PC today, so I lost this Excel spreadsheet. Okay, this one. Okay, so, the good news. Good news, we are on track, right? And here we are, meeting number five. So, today we're going to discuss Whisper API and local installation. The bad news, I took a look at some of the homeworks, like, two or three, maybe five, but not, like, very high level. I like the blog posts you guys made, so I like the pictures, and I think I will share some of my favorite pictures next time. So, I assume I will need this weekend to check them and probably to, I don't know, like, set some marks or something like that. So, just give me some time, but I haven't found any, like, critical problems there. So, just remember to share the workflow, how you did that, like, for the first task. So, I need to see, like, what prompts you've been using. And don't forget about the pictures, so the pictures are necessary for our blog post. So, I think we're going to focus on that, probably, either on this lecture or this one. We don't have time to review the homework. So, today we're going to discuss the Whisper API, and this is very, let me show the slides. So, here are the slides for today. So, what is the Whisper, and why are we discussing that? So, mostly when people discuss the generative AI or AI in general, so they focus on, like, LLMs, I don't know, like, compilers, functions, and stuff like that. So, everything that works with text, right, because it has more application to the business when you work with text. But still, usually what is being forgotten is images and audio processing. And regarding the images, I can't say it's, like, easier to monetize this, so not too much business applications. So, I know a lot of cases where clients came to us, like, to EPAM and asked to create something. Like, I don't know, maybe I read over a hundred of cases. And only several of them were related to image generation. But some of them, like, more of these cases are related to audio. One of the usual, like, requests is, okay, guys, we have 5,000 hours of recordings of, I don't know, meetings or customers calling us for the support. So, we need to understand this data, right? So, imagine you have a first-line support, like, people calling, like, real users calling, describing the issue, and the operators are just sharing. So, what should you do to solve it? And you need to control it somehow, right? So, you need to see what are the most popular questions or what are the answers. So, is the client happy or not, right? And for this case, you need to voice processing. Processing the voice is actually very old technology. I don't remember how old it is, but, like, two things here usually working is text-to-speech and speech-to-text. And if we check, I don't know, like, let's check Azure text-to-speech. Okay, it's AI speech now. All right, I got it. Everything is AI now. Let me check the pricing, actually. DTS pricing. It's super cheap. It's super cheap. Okay, I think here. So, this is, okay, free tier, all right. All right, delete this one. So, speech-to-text. So, in order to transcript something, like, if you have a phone call, real-time transcription, $1 per hour, that's too much. Okay, batch transcription. So, if I have a lot of audio calls, I don't know, like, 100, I can post them as a batch, and this will result in 80 cents per hour. So, not too much, actually. Yeah, as usually, like, Microsoft has, like, very complicated stuff for billing, so it's hard to understand how much you will spend. Yeah, and here is, like, the text-to-speech. So, basically, two directions. Like, from speech, you can extract text, and from text, you can generate speech, right? And this is, like, the standard voice. I think this standard voice become neural, like, during, like, last year, because previously, they had different, they had a different pricing. So, like, the standard voice is, like, this robotized voice you usually hear when you call a bank or something, and neural voice is, like, more pleasant, more realistically, more naturally toned. So, right now, I don't think, they should have some kind of, like, cheap voice, like this old cheap voice. Probably, it's inside the Azure AI studio, but the check is not expensive, right? So, one million characters is, like, it's a lot. $15 for business, it's, like, well, not too much, I think. So, and in case we have 1,000 of, like, phone calls transcripted, so it will result as only, like, less than $200. That's acceptable for that. And they definitely use machine learning there, right? So, because there's no way programmatically, like, algorithmically to transcribe the speech. But still, the interesting thing here is it's also possible to use machine learning techniques and all this concept of token we've been discussing previously, right? In working with speech and understanding the speech. And the project, the project I would like to show you, this Whisper, so let me find, it should be here in the notes I put. Yeah, this one. It was introduced very similar once the chat GPT arrived, right? So, I'm just trying to recognize, what was the time when chat GPT released? Okay, chat GPT released. Okay, November 30, all right, 2022. Yeah, so they introduced Whisper, like, a couple of months before chat GPT. And to know what happens, like, do you remember this movie, 13th Floor? This one. So, do you know this movie, 13th Floor? It's kind of popular from scientific perspective, but they got the problem, right? So, this is the cyber, like, not so cyberpunk, but probably, like, interesting science fiction. But take a look at the release year, it's 1999. The problem is that in 1999, this movie gets out, Matrix. And that's why, like, everyone knows about the Matrix. It was, like, a very successful movie. And this is the reason much more less people know that this movie, 13th Floor, even exists. If they release it, like, a year previous, before Matrix, so much more success. So, the same as Whisper, right? So, chat GPT release, November 13, and Whisper, actually the same company, OpenVA, but it was not so hyped in September as it become in November, right? So, what they introduced is they actually have been working, like, at the same time, looks like they've been working at the Whisper in parallel, like, working with Shazoo PC, right? So, they reused the same, like, not the same, but similar architecture, like, encoders, decoders, and tokens to predict the tokens, but not from text, but from audio as well. So, they just take a look at the audio, right? And in the paper, so they share, like...
this one. So they took 680,000 hours of multilingual audio. So they sliced it in 30 seconds, time like steps, not steps but chunks, and they trained the model against it using the same approach they use for the GPT. So pretty much the same, like slice the data into some samples, try to predict the tokens, like compare with the result, learn, train the model and so on. And this results in a very interesting thing, very high quality. And I don't know if there are any models right now which can beat the quality of Whisper. Because once it was released, it was like very high quality. They released another model a couple of months ago, I think it's called Whisper Turbo, but the architecture seems to stay the same. And one more interesting thing here is it's completely open sourced. So you can download it. And it doesn't require a lot of VRAM to run. So you can run it on the GPU. You can run it a bit slower on the CPU. And as far as it's like not a large language model, it will perform like with acceptable speed. I've been running it at the CPU once and it works normally. So not so slow as the LLM. So we are not so interested in technical details and I don't understand them actually just to explain to you. But what we're interested in is actually we're interested in the result, right? So how it works and how I can use it. So how to use it. So you just install it as a Python package, but you need PyTorch. And this is like why I mentioned the PyTorch previously. So in order to install it locally, so you must have the PyTorch. And this is the... Come on, where is it? It should be PyTorch. Download probably? No. Come on, where's download? Get started. Oh, here, install. Okay. Like if you would like to try VSPR, right? Or in future if you would like to work with PyTorch. So there's like two options here. So it doesn't matter which version you will be installing. You're going to see the same table for every time, like it's a couple of years the same. So if you're installing it like in straightforward, like recommended way, like usually it's Windows, Python, Packager, Pip, Python. And here is like CUDA or CPU. And depending on what you will select, the URL will be different, right? So CUDA is the accelerated computing framework created by NVIDIA, as far as I remember by NVIDIA, to run the computations on the GPU. And if you download and install this PyTorch, you will be running everything against the GPU. But if you do not have GPU, right? So you can install it without like just the CPU. And it will be the same PyTorch. It will work the same, but much more slower, right? But it will still work. So you need to prior to install the PyTorch. So first you need Python. Next, you need to select the CUDA version. So usually the later the better. You just copy this and install. And actually I did that today because my PyTorch, well not PyTorch, but Whisper failed to run today. So I've been preparing to the demo and I also have like the some files, audio files. So and the problem is, so why facing them? And you may face the same. So let me show you. So the problem with Python is that somehow like for modern applications, I use NumPy version two. So NumPy is a framework, is a library to work in like with numbers in Python. And it's required by PyTorch. But this PyTorch and it's like implementation in Whisper doesn't work with NumPy 2.0.2. So it requires like old version. And that's kind of problem you may face as well. So what I did is actually I installed like virtual environment. So I created a virtual Python environment and downloaded once again, PyTorch, Whisper and all the stuff. So at least now it works, right? So that's the problem you may face with working with any software in Python. At least I think there should be some kind of workaround, but taking a look at how many people using the virtual environments, I think this is the acceptable workaround. So getting back here. So what you need is just to install the library and that's it. You can install it from source, but I prefer like this lazy option. And it also requires ffmpeg. So ffmpeg, this is the most popular library to work in with audio and video. So, and actually, this may be useful for you in future. So I know like working in development or just processing media like audio and video requires you to convert files from one format to another, split them, I don't know, change the bitrate and anything. So everything can be done with ffmpeg, but ffmpeg is a command line interface. Like it's a program which allows you to do everything, but without the user interface, without the UI. But you can work with it programmatically. So for example, this is how you can convert from MP4 to AV, like the old format, right? And this is like the greatest software and the fastest solution right now, I think. So a lot of audio and video converting software use ffmpeg under the hood. So it will be necessary as well, if you would like to start with Whisper, because Whisper will need to transform the data from one format to another. So for example, in my examples, I'm gonna use MP3, but in some cases, it may be WAV format or something like that. So Whisper need to transform from one format to another. And here are the models and languages. So as far as it's machine learning model, right? So you can pick up any model and getting back here. So remember, we've been discussing this, like model card, quantization, distillation, and so on. So different models, right? So this is like 13 billion model. This is 70 billion model. The same with Whisper. So this model has 39 million parameters. It's tiny, and it requires only one gigabyte of video RAM, like VRAM. And it's very fast, right? So assuming the large model has like 1.5 billion parameters, it will occupy 10 gigabytes of VRAM. And the speed is one, like one x, right? So this one is 10 times faster, and Turbo is eight times faster. And that's it. They have large V2 and large V3. And what's more interesting here is the quality. So the quality here is measured in WER, like word error rate. So it just evaluating how much, how many words are like mispredicted, like how many words are wrong. And you can see the languages here. So depending on the data set and probably the language structure, this really differs, right? So I'm not surprised the Spanish has like less errors than any other languages, because Spanish, at least from my perspective, I know like a couple of words, and it looks like very easy language, right? Like straightforward and kind of easy. Yeah. And some of the languages probably lacking the presence in the internet will result in like half of the words misspelled, like Belarusian. I don't know. I think I need to, I know Belarusian language, and I think I need to find some podcast in Belarusian and try to feed it into the Whisper and check like how it works. But that's somewhat strange because 42 out of 100, comparing to six in Ukrainian. So Belarusian and Ukrainian are super similar, super similar. So I can understand like 90% of Ukrainian, and Ukrainians understand 90% of Belarusian. Probably it's the problem in the data set, right? Okay. But in case, like in most cases, we're going to work with English, right? So working with these languages is super cool. And command line usage is pretty easy. So let's run the example. And for the example, I have the, I took the Matrix movie, and you know, like there is a scene, Agent Smith is talking with Morpheus. Matrix, Agent Smith. He's delivering the speech about like the humans. I'm not sure you will be able to hear it because I'm using Teams in browser. So it's not desktop application. So I can't share my audio. But here is like the Agent Smith is interrogating Morpheus, and he's delivering the speech like, I got the idea. So human beings are a virus, and we like the agents are the cure. So I will drop you the link, or you can just Google it and reconsider. This is, I picked this sample because like high quality speech and no any like music in the ground and something like that. So let me try to run this and check how it will work. So default Whisperer. We just need to call Whisperer and pass the parameter as the data file. So right now it's detecting the language using first 30 seconds, and the language is detected to English. So you can do that actually manually. You can specify the language, and here is the output, right? So this is like the Smith is discussing it. This is very precise. Actually, this is very precise. I think it's like probably here we have the problems. Yeah, I think it's missing something still. But I don't know what model I'm using. So let me call the usage. Okay, so a lot of things here. So we are interested in the model. So we can specify the model from here, right? So these are the available models. I don't know which one is the default, maybe small or base. So I have a lot of VRAM, so I can actually launch a large model. So let's try with probably another model. And you can also specify the language explicitly. I don't think it will help us because it automatically recognized that this is English. One more thing actually, very interesting thing you can do with Whisperer. So here should be a parameter which is called prompt. Let me find it. Okay, model device output verbose. There should be a parameter which is called something like a prompt. No. Yeah, this one, initial prompt. So the trick is, so by default it's none. But let's get back to the idea. So Whisperer works with tokens, right? And everything, like in this architecture, you can pass the system message. So you can pass the system message in Whisperer. And I've been working once on English transcription task. So we decided to make, to transcribe the English assessments in our company. So there is like an interviewer asking questions and the person replying. And one of the idea was to calculate the, I don't know how it's called, like the words like when the person is thinking, like parasite words. So it's not possible to do with just plain Whisperer, hardly possible. But if you explicitly provide them in a prompt, it will recognize them. And also a lot of times in this dialogue, there were mentioned a company name, Epam. And there is no such word in Whisperer dataset. So that's why it was looking for a similar. But if you provide it in initial prompt and it will like see the same token combination, it will just do not search for any other alternative. So this is very powerful thing, like very underrated. Okay. So let's get back here and let's try the model. Let me try different models and we're going to see the performance like this speed. Let's try with tiny, the smallest model and let's check how fast it will work. Pretty fast. And let me try. Turbo. Okay. Pretty fast as well. I don't see a big difference in speed here, right? At least at this sample, the sample is only one minute. Okay. Yeah. The final token was, took the time and let me try large. So the large should give us the highest quality. Okay. Yeah. You see the difference. So large and turbo versus the tiny. So the tiny, you can just understand what's going on in this discussion. So if you don't care about particular worlds, if you just would like, I don't know, to summarize it in future and make some decision, like, I don't know, sentiment analysis or something. So maybe tiny is okay, but still that's a lot of questions as far as I see. But here the turbo and the large, so they work very effectively. All right. So, and once I run every model, so it's getting downloaded. So I don't think I started small. So probably it will be downloading right now. No, no, it's already downloaded, I think. Yeah. So once I selected large first time, it started downloading all of this model, all of this model, like one and a half gigabytes of model locally. So if you have a lot of files, I don't know, million of hours. So what you just need, you just need the PC. So you can use it in cloud, right? So let's check the pricing, open the icon, API pricing. So you can do that with API. Let me scroll to the Whisper, fine tuning, real time assistance. Okay. Images, audio models. Okay. So what we're going to pay, we're going to pay 0.16 cents per minute. So let me calculate one hour. Oh, come on. So 0.6 cents per minute. So it means one hour will cost us 36 cents. Yeah. So it means like 100 hours of transcription will result in $36. So there should be some point where it's cheaper to purchase hardware and run it on your hardware and save the hardware for yourself instead of sending to the, to the open AI, right? Yeah. So in some cases, it's more effective to do it on your own machine. I didn't know, like spend the night, create some Python scripts and yeah, regarding Python, you can call it from Python as well. So it's Python library. So you can, you can use it from command line, but you can also, so here is the command line usage. It can also import it, load the model and transcribe your own data just as passing as the reference to the, to the file, right? And here are the examples how you can do that with, with Python. Still, it has one problem, which probably been solved in some forks of the Whisper, like Whisper X and so on. The problem is it doesn't differentiate speakers. So you will not see the difference between speaker one and speaker two, right? So what I did in homework, I shared the file with you where there is a difference between speaker one and speaker two. So here is no, no, any difference, but the output formats may be different, right? So let me call the help once again. Here, output format. You can see we have a lot of different output formats, TXT, VTT, SRT, TSV, JSON, all. Let's try, let's try to run with all, but probably all is a default, right? So let me check. Oh, okay. Yeah. All is a default. It means I already have all this information available. So let's, let's check it out. If you like JSONs, you can run this. Let me format it. Very good, right?
